{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a6352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0..3.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23834f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb19ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e00dcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bb8d721ec4400a9903a07d3bcc2600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/knkarthick--dialogsum to C:/Users/mobeenH20/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5384989b0948464da2b6bc73d67d6734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b529540b2e46fcb6eccf13bc5afe06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9664739ea64974a2b82070a7eba980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218c71ed0d274b9496c5f56f31689637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c17636e4e74bd3b84814c29e849402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/mobeenH20/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a92855e48248de84dcfee1c544f531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"knkarthick/dialogsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3b7424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c182f06a0347328fc9cfcde33785f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobeenH20\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mobeenH20\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccf54556536428aae49dbb5251b584c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac91bda60b6445779dca681bf2b1e092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4165e217342a45768696e9bd831c7e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba2866bd13b4400a15d7420cc8c0f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f59ad2803fe43a0a196ad2efb7c4f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a7e466a4bc4409ade48f17ebce10c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c8361",
   "metadata": {},
   "source": [
    "\n",
    "In modern deep learning, it is possible to pull out number of parameters & see which one of them are trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "36da20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    total_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        total_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable Model Params: {trainable_model_params} total Model Params: {total_model_params} percentage of trainable Params: {(trainable_model_params/total_model_params)*100}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a82ec5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainable Model Params: 3538944 total Model Params: 251116800 percentage of trainable Params: 1.4092820552029972'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_model_parameters(original_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed5d01",
   "metadata": {},
   "source": [
    "## PEFT/LoRA model for Fine-Tuning\n",
    "\n",
    "First we need to setup LoRA/PEFT model  for fine-tuning. Instead of fine-tuning all the parameters of LLM, it fine-tunes small number of trainable parameters to reduce computation and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8179f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 32,    # rank: defines the dimension of the adatpter model to be trained\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q', 'v'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3725284d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters: trainable Model Params: 3538944 total Model Params: 251116800 percentage of trainable Params: 1.4092820552029972\n"
     ]
    }
   ],
   "source": [
    "# adding the LoRA layers to the original model\n",
    "\n",
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "print(f\"number of trainable parameters: {trainable_model_parameters(peft_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42001c1c",
   "metadata": {},
   "source": [
    "## Train PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "43d125e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./dialogue-summary-peft-training\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,     #higher learning rate than full-fine tuing\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=3,\n",
    "    max_steps=3\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f3e6d55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobeenH20\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>47.583300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./dialogue-summary-peft-training-save\\\\tokenizer_config.json',\n",
       " './dialogue-summary-peft-training-save\\\\special_tokens_map.json',\n",
       " './dialogue-summary-peft-training-save\\\\spiece.model',\n",
       " './dialogue-summary-peft-training-save\\\\added_tokens.json',\n",
       " './dialogue-summary-peft-training-save\\\\tokenizer.json')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "peft_trainer.model.save_pretrained(\"./dialogue-summary-peft-training-save\")\n",
    "tokenizer.save_pretrained(\"./dialogue-summary-peft-training-save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7bc81129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the trained LoRA/PEFT adapter will be combined with the original FLAN-t5 model \n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    peft_model_base,\n",
    "    \"./dialogue-summary-peft-training-save\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    is_trainable=False            # if we need to prepare the model for further training we set to is_trainable=True. rn its  infering PEFT model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c4a3b3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters: trainable Model Params: 0 total Model Params: 251116800 percentage of trainable Params: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of trainable parameters: {trainable_model_parameters(peft_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b042f7b",
   "metadata": {},
   "source": [
    "## Evaluation of Model (Human Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2e7f2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model trained previously (full-fine-tuned). \n",
    "intruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./dialogue-summary-training-save\", torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ee1b9cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Original Model Summary: You'd like to upgrade your system, but you'd like to make your own flyers and banners.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Instruct Model: #Person1#: You'd probably want to upgrade your computer. #Person2#: You could also upgrade your hardware. #Person1#: You'd probably want a faster processor, more memory and a faster modem. #Person2#: You might want to add a CD-ROM drive too.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT Model: #Person1#: You'd probably want to upgrade your computer. #Person2#: You could also upgrade your hardware. #Person1#: You'd probably want a faster processor, more memory and a faster modem. #Person2#: You might want to add a CD-ROM drive too.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation:\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "generation_config = GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "original_model_output = tokenizer.decode(original_model.generate(input_ids=inputs['input_ids'].to('cuda'), generation_config=generation_config)[0], skip_special_tokens = True)\n",
    "intruct_model_output = tokenizer.decode(intruct_model.generate(input_ids=inputs['input_ids'], generation_config=generation_config)[0], skip_special_tokens = True)\n",
    "peft_model_output = tokenizer.decode(peft_model.generate(input_ids=inputs['input_ids'], generation_config=generation_config)[0], skip_special_tokens = True)\n",
    "\n",
    "\n",
    "print(dash_line)\n",
    "print(f\"Baseline Human Summary: {summary}\")\n",
    "print(dash_line)\n",
    "print(f\"Original Model Summary: {original_model_output}\")\n",
    "print(dash_line)\n",
    "print(f\"Instruct Model: {intruct_model_output}\")\n",
    "print(dash_line)\n",
    "print(f\"PEFT Model: {peft_model_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17602bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b2509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6941ea25",
   "metadata": {},
   "source": [
    "## Evaluation using Rouge (Metric Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "65df664b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3fe7703b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_base_line</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>The memo should go out to all employees by thi...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>#Person1#: This memo is for the intra-office a...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>#Person1#: Ms. Dawson, I need to take a dictat...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>People are talking about the traffic jams in t...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>#Prsect1: I'm finally here. #Prsect1: I got st...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>#Person1#: I got stuck in traffic. #Person2#: ...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>Masha and Hero are divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>Brian's birthday.</td>\n",
       "      <td>#Person1#: Happy Birthday, Brian. #Person2#: I...</td>\n",
       "      <td>Brian's birthday is coming up.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     human_base_line  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  The memo should go out to all employees by thi...   \n",
       "1  #Person1#: This memo is for the intra-office a...   \n",
       "2  #Person1#: Ms. Dawson, I need to take a dictat...   \n",
       "3  People are talking about the traffic jams in t...   \n",
       "4  #Prsect1: I'm finally here. #Prsect1: I got st...   \n",
       "5  #Person1#: I got stuck in traffic. #Person2#: ...   \n",
       "6               Masha and Hero are getting divorced.   \n",
       "7               Masha and Hero are getting divorced.   \n",
       "8                       Masha and Hero are divorced.   \n",
       "9                                  Brian's birthday.   \n",
       "\n",
       "                            instruct_model_summaries  \\\n",
       "0     #Person1#: I need to take a dictation for you.   \n",
       "1     #Person1#: I need to take a dictation for you.   \n",
       "2     #Person1#: I need to take a dictation for you.   \n",
       "3  The traffic jam at the Carrefour intersection ...   \n",
       "4  The traffic jam at the Carrefour intersection ...   \n",
       "5  The traffic jam at the Carrefour intersection ...   \n",
       "6               Masha and Hero are getting divorced.   \n",
       "7               Masha and Hero are getting divorced.   \n",
       "8               Masha and Hero are getting divorced.   \n",
       "9  #Person1#: Happy Birthday, Brian. #Person2#: I...   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  The memo is to be distributed to all employees...  \n",
       "1  The memo is to be distributed to all employees...  \n",
       "2  The memo is to be distributed to all employees...  \n",
       "3  The traffic jam at the Carrefour intersection ...  \n",
       "4  The traffic jam at the Carrefour intersection ...  \n",
       "5  The traffic jam at the Carrefour intersection ...  \n",
       "6               Masha and Hero are getting divorced.  \n",
       "7               Masha and Hero are getting divorced.  \n",
       "8               Masha and Hero are getting divorced.  \n",
       "9                     Brian's birthday is coming up.  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "base_line_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "generation_config = GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation:\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    original_model_output = tokenizer.decode(original_model.generate(inputs['input_ids'].to('cuda'), generation_config=generation_config)[0], skip_special_tokens = True)\n",
    "    original_model_summaries.append(original_model_output)\n",
    "    \n",
    "    intruct_model_output = tokenizer.decode(intruct_model.generate(inputs['input_ids'], generation_config=generation_config)[0], skip_special_tokens = True)\n",
    "    instruct_model_summaries.append(intruct_model_output)\n",
    "    \n",
    "    peft_model_output = tokenizer.decode(peft_model.generate(input_ids=inputs['input_ids'], generation_config=generation_config)[0], skip_special_tokens = True)\n",
    "    peft_model_summaries.append(peft_model_output)\n",
    "    \n",
    "zipped_summaries = list(zip(base_line_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human_base_line', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n",
    "\n",
    "df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "36a3ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model: \n",
      "{'rouge1': 0.2405180862338082, 'rouge2': 0.09032938076416336, 'rougeL': 0.210439169181522, 'rougeLsum': 0.2116263920463764}\n",
      "Instruct Model: \n",
      "{'rouge1': 0.3226723646723646, 'rouge2': 0.1411928370659991, 'rougeL': 0.27076383358992057, 'rougeLsum': 0.27249606271345406}\n",
      "PEFT Model: \n",
      "{'rouge1': 0.30155707905707907, 'rouge2': 0.10492587199223355, 'rougeL': 0.24782560032560028, 'rougeLsum': 0.25171791296791296}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "original_model_results = rouge.compute(\n",
    "        predictions=original_model_summaries, \n",
    "        references=base_line_summaries[0:len(instruct_model_summaries)],\n",
    "        use_aggregator=True,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "        predictions=instruct_model_summaries,\n",
    "        references=base_line_summaries[0:len(instruct_model_summaries)],\n",
    "        use_aggregator=True,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "        predictions=peft_model_summaries,\n",
    "        references=base_line_summaries[0:len(instruct_model_summaries)],\n",
    "        use_aggregator=True,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Original Model: \")\n",
    "print(original_model_results)\n",
    "\n",
    "print(\"Instruct Model: \")\n",
    "print(instruct_model_results)\n",
    "\n",
    "print(\"PEFT Model: \")\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af702ed",
   "metadata": {},
   "source": [
    "#### PEFT model is little less better than Instruct model but it saves alot more computational power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81300808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00358884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
