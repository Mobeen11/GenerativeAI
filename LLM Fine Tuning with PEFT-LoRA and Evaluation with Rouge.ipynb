{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a6352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0..3.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f23834f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb19ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e00dcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/mobeenH20/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ee760eb65d4bdc835d0f65e9efa1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"knkarthick/dialogsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d3b7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c8361",
   "metadata": {},
   "source": [
    "\n",
    "In modern deep learning, it is possible to pull out number of parameters & see which one of them are trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36da20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    total_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        total_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable Model Params: {trainable_model_params} total Model Params: {total_model_params} percentage of trainable Params: {(trainable_model_params/total_model_params)*100}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a82ec5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainable Model Params: 247577856 total Model Params: 247577856 percentage of trainable Params: 100.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_model_parameters(original_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed5d01",
   "metadata": {},
   "source": [
    "## PEFT/LoRA model for Fine-Tuning\n",
    "\n",
    "First we need to setup LoRA/PEFT model  for fine-tuning. Instead of fine-tuning all the parameters of LLM, it fine-tunes small number of trainable parameters to reduce computation and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8179f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(data_instance):\n",
    "    start_prompt = \"Summarize the following dialogue.\\n\\n\"\n",
    "    end_prompt = \"\\n\\nSummary: \"\n",
    "    \n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in data_instance['dialogue']]\n",
    "    \n",
    "    data_instance['input_ids'] = tokenizer(prompt, padding=\"max_length\", \n",
    "                                           truncation=True, return_tensors=\"pt\").input_ids\n",
    "    data_instance['labels'] = tokenizer(data_instance['summary'], padding=\"max_length\", \n",
    "                                           truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return data_instance\n",
    "def tokenize_function(data_instance):\n",
    "    start_prompt = \"Summarize the following dialogue.\\n\\n\"\n",
    "    end_prompt = \"\\n\\nSummary: \"\n",
    "    \n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in data_instance['dialogue']]\n",
    "    \n",
    "    data_instance['input_ids'] = tokenizer(prompt, padding=\"max_length\", \n",
    "                                           truncation=True, return_tensors=\"pt\").input_ids\n",
    "    data_instance['labels'] = tokenizer(data_instance['summary'], padding=\"max_length\", \n",
    "                                           truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return data_instance\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 32,    # rank: defines the dimension of the adatpter model to be trained\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q', 'v'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3725284d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters: trainable Model Params: 3538944 total Model Params: 251116800 percentage of trainable Params: 1.4092820552029972\n"
     ]
    }
   ],
   "source": [
    "# adding the LoRA layers to the original model\n",
    "\n",
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "print(f\"number of trainable parameters: {trainable_model_parameters(peft_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42001c1c",
   "metadata": {},
   "source": [
    "## Train PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "202257cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(data_instance):\n",
    "    start_prompt = \"Summarize the following dialogue.\\n\\n\"\n",
    "    end_prompt = \"\\n\\nSummary: \"\n",
    "    \n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in data_instance['dialogue']]\n",
    "    \n",
    "    data_instance['input_ids'] = tokenizer(prompt, padding=\"max_length\", \n",
    "                                           truncation=True, return_tensors=\"pt\").input_ids\n",
    "    data_instance['labels'] = tokenizer(data_instance['summary'], padding=\"max_length\", \n",
    "                                           truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return data_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a1fc2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\mobeenH20\\.cache\\huggingface\\datasets\\knkarthick___csv\\knkarthick--dialogsum-cd36827d3490488d\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-66f989879bb1839a.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\mobeenH20\\.cache\\huggingface\\datasets\\knkarthick___csv\\knkarthick--dialogsum-cd36827d3490488d\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-0c39af25fee56ecc.arrow\n"
     ]
    }
   ],
   "source": [
    "# to handle dataset for all batches and splits\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43d125e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./dialogue-summary-peft-training\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,     #higher learning rate than full-fine tuing\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=3,\n",
    "    max_steps=3\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3e6d55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobeenH20\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:15, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>46.833300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./dialogue-summary-peft-training-save\\\\tokenizer_config.json',\n",
       " './dialogue-summary-peft-training-save\\\\special_tokens_map.json',\n",
       " './dialogue-summary-peft-training-save\\\\spiece.model',\n",
       " './dialogue-summary-peft-training-save\\\\added_tokens.json',\n",
       " './dialogue-summary-peft-training-save\\\\tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "peft_trainer.model.save_pretrained(\"./dialogue-summary-peft-training-save\")\n",
    "tokenizer.save_pretrained(\"./dialogue-summary-peft-training-save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bc81129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the trained LoRA/PEFT adapter will be combined with the original FLAN-t5 model \n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    peft_model_base,\n",
    "    \"./dialogue-summary-peft-training-save\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    is_trainable=True            # if we need to prepare the model for further training we set to is_trainable=True. rn its  infering PEFT model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4a3b3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters: trainable Model Params: 3538944 total Model Params: 251116800 percentage of trainable Params: 1.4092820552029972\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of trainable parameters: {trainable_model_parameters(peft_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b042f7b",
   "metadata": {},
   "source": [
    "## Evaluation of Model (Human Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e7f2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model trained previously (full-fine-tuned). \n",
    "intruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./dialogue-summary-training-save\", torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee1b9cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Original Model Summary: #Person1#: I'm not sure what exactly I would need to upgrade my software. #Person1#: I'm not sure what exactly I would need to upgrade my hardware. #Person1#: I'm not sure what exactly I would need to upgrade my hardware. #Person1#: I'm not sure what exactly I would need to upgrade my hardware. #Person2#: I'm not sure what I would need to upgrade my hardware. #Person1#: I'm not sure what I would need to upgrade my software. #Person1#: I'm not sure what I would need to upgrade my hardware. #Person2#: I might also want to upgrade my hardware. #Person1#: You might want to add a CD-ROM drive.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Instruct Model: #Person1#: You'd probably want to upgrade your computer. #Person2#: You could also upgrade your hardware. #Person1#: You'd probably want a faster processor, more memory and a faster modem. #Person2#: You might want to add a CD-ROM drive too.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT Model: #Person1#: You'd probably want to upgrade your computer. #Person2#: You could also upgrade your hardware. #Person1#: You'd probably want a faster processor, more memory and a faster modem. #Person2#: You might want to add a CD-ROM drive too.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation:\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "generation_config = GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "original_model_output = tokenizer.decode(original_model.generate(input_ids=inputs['input_ids'].to('cuda'), generation_config=generation_config)[0], skip_special_tokens = True)\n",
    "intruct_model_output = tokenizer.decode(intruct_model.generate(input_ids=inputs['input_ids'], generation_config=generation_config)[0], skip_special_tokens = True)\n",
    "peft_model_output = tokenizer.decode(peft_model.generate(input_ids=inputs['input_ids'], generation_config=generation_config)[0], skip_special_tokens = True)\n",
    "\n",
    "\n",
    "dash_line = \"-\".join(\"\" for x in range(0,100))\n",
    "print(dash_line)\n",
    "print(f\"Baseline Human Summary: {summary}\")\n",
    "print(dash_line)\n",
    "print(f\"Original Model Summary: {original_model_output}\")\n",
    "print(dash_line)\n",
    "print(f\"Instruct Model: {intruct_model_output}\")\n",
    "print(dash_line)\n",
    "print(f\"PEFT Model: {peft_model_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17602bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b2509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6941ea25",
   "metadata": {},
   "source": [
    "## Evaluation using Rouge (Metric Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65df664b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fe7703b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_base_line</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>Ms. Dawson, please take dictation.</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>#Person1: This is an intra-office memo. #Perso...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>Message to all employees by the President this...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you.</td>\n",
       "      <td>The memo is to be distributed to all employees...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>The person who got stuck in traffic is a car d...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>The person is a little late, but he's finally ...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>The driver of a car is having a problem.</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>#Person1: #Person2: What a shame! #Person1: We...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>Masha and Hero are getting married.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>Brian is celebrating his birthday.</td>\n",
       "      <td>#Person1#: Happy Birthday, Brian. #Person2#: I...</td>\n",
       "      <td>Brian's birthday is coming up.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     human_base_line  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0                 Ms. Dawson, please take dictation.   \n",
       "1  #Person1: This is an intra-office memo. #Perso...   \n",
       "2  Message to all employees by the President this...   \n",
       "3  The person who got stuck in traffic is a car d...   \n",
       "4  The person is a little late, but he's finally ...   \n",
       "5           The driver of a car is having a problem.   \n",
       "6               Masha and Hero are getting divorced.   \n",
       "7  #Person1: #Person2: What a shame! #Person1: We...   \n",
       "8                Masha and Hero are getting married.   \n",
       "9                 Brian is celebrating his birthday.   \n",
       "\n",
       "                            instruct_model_summaries  \\\n",
       "0     #Person1#: I need to take a dictation for you.   \n",
       "1     #Person1#: I need to take a dictation for you.   \n",
       "2     #Person1#: I need to take a dictation for you.   \n",
       "3  The traffic jam at the Carrefour intersection ...   \n",
       "4  The traffic jam at the Carrefour intersection ...   \n",
       "5  The traffic jam at the Carrefour intersection ...   \n",
       "6               Masha and Hero are getting divorced.   \n",
       "7               Masha and Hero are getting divorced.   \n",
       "8               Masha and Hero are getting divorced.   \n",
       "9  #Person1#: Happy Birthday, Brian. #Person2#: I...   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  The memo is to be distributed to all employees...  \n",
       "1  The memo is to be distributed to all employees...  \n",
       "2  The memo is to be distributed to all employees...  \n",
       "3  The traffic jam at the Carrefour intersection ...  \n",
       "4  The traffic jam at the Carrefour intersection ...  \n",
       "5  The traffic jam at the Carrefour intersection ...  \n",
       "6               Masha and Hero are getting divorced.  \n",
       "7               Masha and Hero are getting divorced.  \n",
       "8               Masha and Hero are getting divorced.  \n",
       "9                     Brian's birthday is coming up.  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "base_line_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "generation_config = GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation:\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    original_model_output = tokenizer.decode(original_model.generate(inputs['input_ids'].to('cuda'), generation_config=generation_config)[0], skip_special_tokens = True)\n",
    "    original_model_summaries.append(original_model_output)\n",
    "    \n",
    "    intruct_model_output = tokenizer.decode(intruct_model.generate(inputs['input_ids'], generation_config=generation_config)[0], skip_special_tokens = True)\n",
    "    instruct_model_summaries.append(intruct_model_output)\n",
    "    \n",
    "    peft_model_output = tokenizer.decode(peft_model.generate(input_ids=inputs['input_ids'], generation_config=generation_config)[0], skip_special_tokens = True)\n",
    "    peft_model_summaries.append(peft_model_output)\n",
    "    \n",
    "zipped_summaries = list(zip(base_line_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human_base_line', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n",
    "\n",
    "df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36a3ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model: \n",
      "{'rouge1': 0.1937527185828484, 'rouge2': 0.05396814464055102, 'rougeL': 0.16514758907387836, 'rougeLsum': 0.16799179815988158}\n",
      "Instruct Model: \n",
      "{'rouge1': 0.3226723646723646, 'rouge2': 0.1411928370659991, 'rougeL': 0.27076383358992057, 'rougeLsum': 0.27249606271345406}\n",
      "PEFT Model: \n",
      "{'rouge1': 0.30155707905707907, 'rouge2': 0.10492587199223355, 'rougeL': 0.24782560032560028, 'rougeLsum': 0.25171791296791296}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "original_model_results = rouge.compute(\n",
    "        predictions=original_model_summaries, \n",
    "        references=base_line_summaries[0:len(instruct_model_summaries)],\n",
    "        use_aggregator=True,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "        predictions=instruct_model_summaries,\n",
    "        references=base_line_summaries[0:len(instruct_model_summaries)],\n",
    "        use_aggregator=True,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "        predictions=peft_model_summaries,\n",
    "        references=base_line_summaries[0:len(instruct_model_summaries)],\n",
    "        use_aggregator=True,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Original Model: \")\n",
    "print(original_model_results)\n",
    "\n",
    "print(\"Instruct Model: \")\n",
    "print(instruct_model_results)\n",
    "\n",
    "print(\"PEFT Model: \")\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af702ed",
   "metadata": {},
   "source": [
    "#### PEFT model is little less better than Instruct model but it saves alot more computational power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81300808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00358884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
