{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mobeen.ahmed%40axxessio.com:****@artifactory.devops.telekom.de/artifactory/api/pypi/magenta-voice-magenta-pypi-virtual/simple, https://pypi.org/simple\n",
      "Requirement already satisfied: pip in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (23.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 401 Error, Credentials not correct for https://artifactory.devops.telekom.de/artifactory/api/pypi/magenta-voice-magenta-pypi-virtual/simple/pip/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://mobeen.ahmed%40axxessio.com:****@artifactory.devops.telekom.de/artifactory/api/pypi/magenta-voice-magenta-pypi-virtual/simple, https://pypi.org/simpleNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: transformers==4.27.2 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.27.2)\n",
      "Requirement already satisfied: datasets==2.11.0 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.27.2) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\mobeen.ahmed\\appdata\\roaming\\python\\python38\\site-packages (from transformers==4.27.2) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.27.2) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.27.2) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.27.2) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.27.2) (2022.4.24)\n",
      "Requirement already satisfied: requests in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.27.2) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.27.2) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers==4.27.2) (4.64.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from datasets==2.11.0) (14.0.2)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from datasets==2.11.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from datasets==2.11.0) (1.4.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from datasets==2.11.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from datasets==2.11.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from fsspec[http]>=2021.11.1->datasets==2.11.0) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from datasets==2.11.0) (3.9.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from datasets==2.11.0) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from aiohttp->datasets==2.11.0) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from aiohttp->datasets==2.11.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from aiohttp->datasets==2.11.0) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from aiohttp->datasets==2.11.0) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from aiohttp->datasets==2.11.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from aiohttp->datasets==2.11.0) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.2) (4.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging>=20.0->transformers==4.27.2) (3.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers==4.27.2) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers==4.27.2) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers==4.27.2) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers==4.27.2) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm>=4.27->transformers==4.27.2) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->datasets==2.11.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->datasets==2.11.0) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mobeen.ahmed\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.11.0) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Load dataset for generating a summary of a dialogue with pre-trained LLM FLAN-T5 from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "huggingface_dataset = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# how the dataset looks like?\n",
    "\n",
    "examples = [40, 200]\n",
    "\n",
    "# print the dash lines to separate the data\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(examples):\n",
    "    print(dash_line)\n",
    "    print(\"Example: \", i)\n",
    "    print(dataset['train'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print(\"Human Summary:\")\n",
    "    print(dataset['train'][index]['summary'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FLAN T-5 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"What time is it?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "            sentence_encoded['input_ids'][0],\n",
    "            skip_special_tokens=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Prompt Example:  0\n",
      "\n",
      "Generate summary of following conversation: \n",
      "#Person1#: I just bought a new dress. What do you think of it?\n",
      "#Person2#: You look really great in it. So are you going to a job interview or a party?\n",
      "#Person1#: No, I was invited to give a talk in my school.\n",
      "#Person2#: So how much did you pay for it?\n",
      "#Person1#: I pay just $70 for it. I saved $30.\n",
      "#Person2#: That's really a bargain.\n",
      "#Person1#: You're right. Well, what did you do while I was out shopping?\n",
      "#Person2#: I watched TV for a while and then I did some reading. It wasn't a very interesting book so I just read a few pages. Then I took a shower.\n",
      "#Person1#: I thought you said you were going to see Mike.\n",
      "#Person2#: I'll go and visit him at his home tomorrow. He'll return home tomorrow morning.\n",
      "#Person1#: I'm glad he can finally returned home after that accident.\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary:\n",
      "While #Person1# made a bargain to buy a new dress, #Person2# watched TV, read a boring book, and took a shower at home.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "**Model Generated Summary with 0 shot Learning:**\n",
      "Person1 is going to a job interview and a party.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Prompt Example:  1\n",
      "\n",
      "Generate summary of following conversation: \n",
      "#Person1#: What do you want to know about me?\n",
      "#Person2#: How about your academic records at college?\n",
      "#Person1#: The average grade of all my courses is above 85.\n",
      "#Person2#: In which subject did you get the highest marks?\n",
      "#Person1#: In mathematics I got a 98.\n",
      "#Person2#: Have you received any scholarships?\n",
      "#Person1#: Yes, I have, and three times in total.\n",
      "#Person2#: Have you been a class leader?\n",
      "#Person1#: I have been a class commissary in charge of studies for two years.\n",
      "#Person2#: Did you join in any club activities?\n",
      "#Person1#: I was an aerobics team member in college.\n",
      "#Person2#: What sport are you good at?\n",
      "#Person1#: I am good at sprint and table tennis.\n",
      "#Person2#: You are excellent.\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary:\n",
      "#Person2# asks #Person1# several questions, like academic records, the highest marks, scholarships, club activities, and skilled sports.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "**Model Generated Summary with 0 shot Learning:**\n",
      "#Person1: How are your academic records at college? #Person2: How many times have you received a scholarship? #Person1: I have. #Person2: Have you been a class leader? #\n"
     ]
    }
   ],
   "source": [
    "# using instruction prompt \n",
    "\n",
    "for i, index in enumerate(examples):\n",
    "    \n",
    "    dialogue = dataset['train'][index]['dialogue']\n",
    "    human_summary = dataset['train'][index]['summary']\n",
    "    \n",
    "    # you can play around with prompt to see the different results\n",
    "    prompt = f\"\"\"\n",
    "Generate summary of following conversation: \n",
    "{dialogue}\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    \n",
    "    # load the dialogue to model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = tokenizer.decode(\n",
    "            model.generate(inputs['input_ids'],\n",
    "            max_new_tokens=50,\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "     \n",
    "    print(dash_line)\n",
    "    print(\"Prompt Example: \", i)\n",
    "    print(prompt)\n",
    "    print(dash_line)\n",
    "    print(\"Human Summary:\")\n",
    "    print(dataset['train'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print(\"**Model Generated Summary with 0 shot Learning:**\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Shot Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_index = [40]\n",
    "example_index_summary = 200\n",
    "\n",
    "one_shot_learning = geneate_prompt(dataset['test'], example_index, example_index_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "**Model Generated Summary with 1 shot Learning:**\n",
      "#Person1# wants to upgrade his system. #Person2#: I'm not sure what exactly I would need. #Person1#: You could consider adding a painting program to your software. #Person2\n"
     ]
    }
   ],
   "source": [
    "human_summary = dataset['test'][index]['summary']\n",
    "\n",
    "# load the dialogue to model with one-shot prompt\n",
    "inputs = tokenizer(one_shot_learning, return_tensors=\"pt\")\n",
    "output = tokenizer.decode(\n",
    "        model.generate(inputs['input_ids'],\n",
    "        max_new_tokens=50,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "\n",
    "print(dash_line)\n",
    "print(\"Human Summary:\")\n",
    "print(dataset['test'][index]['summary'])\n",
    "print(dash_line)\n",
    "print(\"**Model Generated Summary with 1 shot Learning:**\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In one-shot learning we feed one sample summary to model as an example.\n",
    "\n",
    "def geneate_prompt(dataset, example_indexs, example_index_summary):\n",
    "    prompt = \"\"\n",
    "    for index in example_indexs:\n",
    "        dialogue = dataset[index]['dialogue']\n",
    "        human_summary = dataset[index]['summary']\n",
    "        \n",
    "        # you can play around with prompt to see the different results\n",
    "        # the stop sequence '{summary}\\n\\n\\n' is important to FLAN t5 model.\n",
    "        prompt += f\"\"\"\n",
    "Dialogue: \n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summarize the above dialogue:\n",
    "{human_summary}\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "        # actual dialogue without summary. Which we want model to summarize\n",
    "        dialogue = dataset[example_index_summary]['dialogue']\n",
    "        prompt += f\"\"\"\n",
    "Dialogue: \n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summarize the above dialogue:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_index = [40, 80, 120]\n",
    "example_index_summary = 200\n",
    "\n",
    "few_shot_prompts = geneate_prompt(dataset['train'], example_index, example_index_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary:\n",
      "#Person2# asks #Person1# several questions, like academic records, the highest marks, scholarships, club activities, and skilled sports.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "**Model Generated Summary with 1 shot Learning:**\n",
      "#Person1 wants to know about his academic records at college. #Person1: The average grade of all his courses is above 85. #Person2: In which subject did you get the highest marks? #Person1:\n"
     ]
    }
   ],
   "source": [
    "human_summary = dataset['train'][example_index_summary]['summary']\n",
    "\n",
    "# load the dialogue to model with few-shot prompts\n",
    "inputs = tokenizer(few_shot_prompts, return_tensors=\"pt\")\n",
    "output = tokenizer.decode(\n",
    "        model.generate(inputs['input_ids'],\n",
    "        max_new_tokens=50,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "\n",
    "print(dash_line)\n",
    "print(\"Human Summary:\")\n",
    "print(human_summary)\n",
    "print(dash_line)\n",
    "print(\"**Model Generated Summary with 1 shot Learning:**\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Configuration of the Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Human Summary:\n",
      "#Person2# asks #Person1# several questions, like academic records, the highest marks, scholarships, club activities, and skilled sports.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "**Model Generated Summary with few shot Learning:**\n",
      "#Person1 wants to know about his academic records at college. #Person1: The average grade of all his courses is above 85. #Person2: In which subject did you get the highest marks? #Person1:\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    config: \n",
    "    max_new_token: setting the output token.\n",
    "    do_sample: provide more flexible output with temperature vairability\n",
    "    temperature: creativity of the model\n",
    "\"\"\"\n",
    "\n",
    "# general_config = GenerationConfig(max_new_tokens=50)\n",
    "general_config = GenerationConfig(max_new_tokens=50, do_sample=False, temperature=1.0)\n",
    "# general_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "# general_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=2.0)\n",
    "\n",
    "\n",
    "human_summary = dataset['train'][example_index_summary]['summary']\n",
    "\n",
    "# load the dialogue to model with few-shot prompts\n",
    "inputs = tokenizer(few_shot_prompts, return_tensors=\"pt\")\n",
    "output = tokenizer.decode(\n",
    "        model.generate(inputs['input_ids'],\n",
    "        generation_config=general_config,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "\n",
    "print(dash_line)\n",
    "print(\"Human Summary:\")\n",
    "print(human_summary)\n",
    "print(dash_line)\n",
    "print(\"**Model Generated Summary with few shot Learning:**\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
