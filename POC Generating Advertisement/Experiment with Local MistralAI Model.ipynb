{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4c90419",
   "metadata": {},
   "outputs": [],
   "source": [
    " # !pip install peft transformers accelerate bitsandbytes langchain\n",
    "# !pip install langchain\n",
    "# !pip install --upgrade ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe23697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobeen.ahmed\\anaconda3\\envs\\experiment_env\\Lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel   # try QLora\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71727db1",
   "metadata": {},
   "source": [
    "### Loading Mistral Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42671999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec63233e38649b1a0cb1468b55a3d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizer: Not Quantized Model\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Add a quantized model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"D:\\\\DemoProjects\\\\GenerativeAI\\\\Labs\\\\data\\\\base_models\\\\\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\"D:\\\\DemoProjects\\\\GenerativeAI\\\\Labs\\\\data\\\\base_models\\\\\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eadf72c-3e3d-4801-b7b6-f1dfc7bc63af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a010b65-b1fb-4857-b44a-0d1d6eaa9cc3",
   "metadata": {},
   "source": [
    "### Trainable Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd882ff-ad85-446e-a0a7-35660ce84095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    total_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        total_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable Model Params: {trainable_model_params}\\ntotal Model Params: {total_model_params}\\npercentage of trainable Params: {100*(trainable_model_params/total_model_params):.3f}%\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b934752-51ba-4776-beac-23fff02ac23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable Model Params: 7241732096\n",
      "total Model Params: 7241732096\n",
      "percentage of trainable Params: 100.000%\n"
     ]
    }
   ],
   "source": [
    "print(trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1490ce-da05-4f61-93d3-50f16d9093eb",
   "metadata": {},
   "source": [
    "### Mistral Text Generation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0591e4e8-46ba-4e67-87c2-14193e767167",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d3fd8f-e7f1-4cbb-a057-eeac537fe559",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23a9c1e9-c85f-42cc-a453-3d39c4f472ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "inputs_not_chat = tokenizer.encode_plus(\"[INST] I want to fly to Paris. Tell me about its weather? [/INST]\", return_tensors=\"pt\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc5d737c-942c-4f91-be11-945cef438145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<s> [INST] I want to fly to Paris. Tell me about its weather? [/INST] Paris, the capital city of France, is known for its mild and changeable weather. The climate in Paris is generally considered to be oceanic, which means that it experiences mild winters and cool summers. The average temperature in Paris hovers around 12 degrees Celsius (54 degrees Fahrenheit) throughout the year, with temperatures ranging from 1 degree Celsius (34 degrees Fahrenheit) in winter to 19 degrees Celsius (66 degrees Fahrenheit) in summer.\\n\\nParis experiences four distinct seasons. In the spring (March to May), the city comes alive with blooming flowers and mild temperatures. In the summer (June to August), temperatures can rise, and the city experiences long days with extended hours of sunlight. Autumn (September to November) brings crisp weather and falling leaves, while winter (December to February) can be wet and chilly, with occasional snowfall.\\n\\nIt's always a good idea to check the current weather forecast before planning your trip to Paris. Keep in mind that the city can experience rain throughout the year, so be sure to pack accordingly. Whatever the season, Paris is a beautiful city that offers many things to see and do, making it a popular destination for travelers from around the world.</s>\"]\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(inputs_not_chat, \n",
    "                               max_new_tokens=1000, \n",
    "                               do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8e718-778b-4638-af41-1843d8df3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "[INST]\n",
    "Instruction: Generate brief summary of the weather for the {city}:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\", \"city\"],\n",
    "    template=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c23df3-8171-4f3d-b810-422ccf4780c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use simple generatting flow\n",
    "original_model.to('cuda')\n",
    "\n",
    "inputs = tokenizer(prompt_template, return_tensors=\"pt\").to(device)\n",
    "outputs = original_model.generate(**inputs, max_new_tokens=5000, pad_token_id=tokenizer.eos_token_id, do_sample=True)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca6dcd-569c-4f9e-b528-75a9d68a8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13c637-4fd9-4db7-8ee6-82ae8f7e0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = input(\"Enter your destination city: \")\n",
    "weather_data = get_weather(OPENWEATHERMAP_API_KEY, city)\n",
    "\n",
    "# retrieved_results = retrieve_vector_db(query)[0][0]\n",
    "\n",
    "rag_chain = ( \n",
    " {\"context\": lambda x: weather_data, \"question\": RunnablePassthrough(), \"city\":lambda x: city}\n",
    "    | llm_chain\n",
    ")\n",
    "rag_chain.invoke(f\"I am traveling to {city}. Tell me about its weather?\")['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
